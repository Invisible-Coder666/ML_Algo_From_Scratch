{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is a L2 Based Regularization technique in which regularization term is the sum of square of all feature weights + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It shrinks the parameters, therefore it is mostly used to prevent multicollinearity.\n",
    "### It reduces the model complexity by coefficient shrinkage, but it does not make coefficient to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class l2_regularization:\n",
    "    def __init__(self,alpha):\n",
    "        self.alpha = alpha\n",
    "    def __call__(self,w):\n",
    "        return self.alpha *.5* (w.T.dot(w))\n",
    "    def grad(self,w):\n",
    "        return self.alpha*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    def __init__(self,alpha=.001,learning_rate=.00001,max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate =learning_rate\n",
    "        # object of l2 regularization\n",
    "        self.regularization = l2_regularization(self.alpha)\n",
    "        \n",
    "    def _init_weight(self):\n",
    "        n_samples,n_features = self.X.shape\n",
    "        self.W = np.zeros((n_features))\n",
    "    \n",
    "    def _add_constant(self):\n",
    "        self.X = np.insert(self.X,0,1,axis=1) \n",
    "       \n",
    "    def fit(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        # adding constant\n",
    "        self._add_constant()\n",
    "        # initialize Weights vector\n",
    "        self._init_weight()\n",
    "        #iterate until max_iter\n",
    "        for _ in range(self.max_iter):\n",
    "            y_pred = self.X.dot(self.W)\n",
    "#             cost_error = 0.5*np.sum((self.Y - y_pred)**2)+ self.regularization(self.W)\n",
    "            derivative_of_cost= -(self.Y - y_pred).dot(self.X) + self.regularization.grad(self.W)\n",
    "            self.W-=self.learning_rate*derivative_of_cost\n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>independent_variable</th>\n",
       "      <th>dependent_variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.502345</td>\n",
       "      <td>31.707006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.426804</td>\n",
       "      <td>68.777596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.530358</td>\n",
       "      <td>62.562382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.475640</td>\n",
       "      <td>71.546632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.813208</td>\n",
       "      <td>87.230925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   independent_variable  dependent_variable\n",
       "0             32.502345           31.707006\n",
       "1             53.426804           68.777596\n",
       "2             61.530358           62.562382\n",
       "3             47.475640           71.546632\n",
       "4             59.813208           87.230925"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data=pd.read_csv('gradient_practise.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['independent_variable'].values.reshape(-1,1)\n",
    "y=data['dependent_variable']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):\n",
    "    \n",
    "    if feature_is_constant == True:\n",
    "        derivative = 2 * np.dot(errors, feature)\n",
    "        \n",
    "    else:\n",
    "        derivative = 2 * np.dot(errors, feature) + 2*(l2_penalty*weight)\n",
    "        \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations=100):\n",
    "\n",
    "    weights = np.array(initial_weights) \n",
    "    feature_matrix = np.insert(feature_matrix,0,1,axis=1) \n",
    "    while max_iterations > 0:\n",
    "    \n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        \n",
    "        errors = predictions - output\n",
    "        \n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "\n",
    "            if i == 0:\n",
    "                feature_is_constant = True\n",
    "            else:\n",
    "                feature_is_constant = False\n",
    "                \n",
    "            derivative = feature_derivative_ridge(errors, feature_matrix[:,i], weights[i], l2_penalty, feature_is_constant)\n",
    "\n",
    "            weights[i] = weights[i] - (step_size * derivative)\n",
    "            \n",
    "        max_iterations -= 1        \n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.array([0., 0.])\n",
    "step_size = 1e-12\n",
    "max_iterations=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (80,2) not aligned: 2 (dim 0) != 80 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-ec724018ee1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m simple_weights_0_penalty = ridge_regression_gradient_descent(X_train, y_train, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                              \u001b[0minitial_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                              0.0, max_iterations)\n\u001b[1;32m      4\u001b[0m \u001b[0msimple_weights_0_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-20ffc29fe00c>\u001b[0m in \u001b[0;36mridge_regression_gradient_descent\u001b[0;34m(feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mmax_iterations\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-245-6869e31c5c48>\u001b[0m in \u001b[0;36mpredict_output\u001b[0;34m(feature_matrix, weights)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,) and (80,2) not aligned: 2 (dim 0) != 80 (dim 0)"
     ]
    }
   ],
   "source": [
    "simple_weights_0_penalty = ridge_regression_gradient_descent(X_train, y_train, \n",
    "                                                             initial_weights, step_size, \n",
    "                                                             0.0, max_iterations)\n",
    "simple_weights_0_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
